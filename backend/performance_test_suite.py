#!/usr/bin/env python3
"""
Comprehensive Performance Testing Suite for RocketTrainer ML API.

This suite tests the ML API under various load conditions to ensure
production readiness with <500ms response times and proper scaling.
"""

import asyncio
import aiohttp
import time
import json
import statistics
import sys
import os
from datetime import datetime, timedelta
from uuid import uuid4, UUID
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor
import threading

# Add the app directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

class PerformanceTestSuite:
    """Comprehensive performance testing for ML API endpoints."""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.ml_api_base = f"{base_url}/api/ml"
        self.results = {
            "single_request_performance": {},
            "concurrent_load_tests": {},
            "cache_performance": {},
            "database_performance": {},
            "system_resource_usage": {},
            "error_rate_analysis": {}
        }
        self.test_user_ids = []
        
    def log_result(self, category: str, test_name: str, metrics: Dict[str, Any]):
        """Log performance test results."""
        if category not in self.results:
            self.results[category] = {}
        
        self.results[category][test_name] = {
            **metrics,
            "timestamp": datetime.now().isoformat()
        }
        
        # Print immediate feedback
        avg_time = metrics.get('avg_response_time_ms', 0)
        success_rate = metrics.get('success_rate', 0) * 100
        status = "‚úÖ" if avg_time < 500 and success_rate >= 95 else "‚ö†Ô∏è" if avg_time < 1000 else "‚ùå"
        
        print(f"{status} {test_name}: {avg_time:.1f}ms avg, {success_rate:.1f}% success")
    
    async def create_test_users_with_data(self, count: int = 10) -> List[UUID]:
        """Create test users with sample match data for performance testing."""
        print(f"üèóÔ∏è  Creating {count} test users with sample data...")
        
        try:
            from app.database import get_db
            from app.models.user import User
            from app.models.match import Match
            
            db = next(get_db())
            user_ids = []
            
            for i in range(count):
                # Create test user
                user_id = uuid4()
                test_user = User(
                    id=user_id,
                    username=f"perf_test_user_{i}_{user_id.hex[:8]}",
                    email=f"perf_test_{i}_{user_id.hex[:8]}@example.com",
                    steam_id=f"steam_perf_{user_id.hex[:8]}",
                    current_rank="platinum_ii",
                    mmr=800 + (i * 10),
                    platform="steam",
                    is_active=True
                )
                
                db.add(test_user)
                user_ids.append(user_id)
                
                # Create sample matches for each user
                base_date = datetime.utcnow() - timedelta(days=30)
                
                for j in range(15):  # More matches for better ML analysis
                    match_date = base_date + timedelta(days=j * 2, hours=j)
                    
                    test_match = Match(
                        id=uuid4(),
                        user_id=user_id,
                        match_date=match_date,
                        processed=True,
                        playlist="ranked-doubles",
                        duration=300 + (j * 5),
                        score_team_0=3 if j % 2 == 0 else 2,
                        score_team_1=2 if j % 2 == 0 else 3,
                        result="win" if j % 2 == 0 else "loss",
                        score=400 + (j * 30),
                        goals=max(0, j // 3),
                        saves=max(0, (j + 1) // 4),
                        assists=max(0, j // 5),
                        shots=max(1, j // 2),
                        boost_usage=max(0.0, 100.0 - (j * 3.0)),
                        average_speed=45.0 + (j * 1.5),
                        time_supersonic=10.0 + j,
                        time_on_ground=70.0 - j,
                        time_low_air=15.0 + (j % 3),
                        time_high_air=5.0 + (j % 4)
                    )
                    
                    db.add(test_match)
            
            db.commit()
            self.test_user_ids = user_ids
            print(f"‚úÖ Created {count} test users with {count * 15} matches")
            return user_ids
            
        except Exception as e:
            print(f"‚ùå Failed to create test data: {e}")
            if 'db' in locals():
                db.rollback()
            return []
    
    async def single_request_performance_test(self):
        """Test individual endpoint performance."""
        print("\nüìä Testing Single Request Performance...")
        
        if not self.test_user_ids:
            print("‚ùå No test users available")
            return
        
        test_user_id = str(self.test_user_ids[0])
        
        async with aiohttp.ClientSession() as session:
            # Test model status endpoint
            await self._test_endpoint_performance(
                session, "model_status", "GET", f"{self.ml_api_base}/model-status"
            )
            
            # Test weakness analysis endpoint
            weakness_payload = {
                "user_id": test_user_id,
                "include_confidence": True,
                "analysis_depth": "detailed"
            }
            await self._test_endpoint_performance(
                session, "weakness_analysis", "POST", 
                f"{self.ml_api_base}/analyze-weaknesses", weakness_payload
            )
            
            # Test training recommendations endpoint
            training_payload = {
                "user_id": test_user_id,
                "skill_level": "platinum",
                "max_recommendations": 5,
                "categories": ["shooting", "aerials"],
                "exclude_completed": True
            }
            await self._test_endpoint_performance(
                session, "training_recommendations", "POST",
                f"{self.ml_api_base}/recommend-training", training_payload
            )
    
    async def _test_endpoint_performance(self, session: aiohttp.ClientSession, 
                                       endpoint_name: str, method: str, url: str, 
                                       payload: Optional[Dict] = None, iterations: int = 10):
        """Test performance of a single endpoint."""
        response_times = []
        success_count = 0
        
        for i in range(iterations):
            start_time = time.time()
            
            try:
                if method == "GET":
                    async with session.get(url) as response:
                        await response.text()
                        status_code = response.status
                else:  # POST
                    async with session.post(url, json=payload) as response:
                        await response.text()
                        status_code = response.status
                
                response_time = (time.time() - start_time) * 1000  # Convert to ms
                response_times.append(response_time)
                
                if status_code in [200, 400, 404]:  # Accept expected error codes
                    success_count += 1
                    
            except Exception as e:
                print(f"   Request {i+1} failed: {e}")
                response_times.append(5000)  # 5 second timeout penalty
        
        # Calculate metrics
        metrics = {
            "avg_response_time_ms": statistics.mean(response_times),
            "median_response_time_ms": statistics.median(response_times),
            "p95_response_time_ms": sorted(response_times)[int(0.95 * len(response_times))],
            "p99_response_time_ms": sorted(response_times)[int(0.99 * len(response_times))],
            "min_response_time_ms": min(response_times),
            "max_response_time_ms": max(response_times),
            "success_rate": success_count / iterations,
            "total_requests": iterations
        }
        
        self.log_result("single_request_performance", endpoint_name, metrics)
    
    async def concurrent_load_test(self, concurrent_users: int = 50, requests_per_user: int = 5):
        """Test API under concurrent load."""
        print(f"\nüöÄ Testing Concurrent Load: {concurrent_users} users, {requests_per_user} requests each...")

        # Adjust concurrent users to available test users
        available_users = len(self.test_user_ids)
        if available_users < concurrent_users:
            concurrent_users = available_users
            print(f"‚ÑπÔ∏è  Adjusted to {concurrent_users} concurrent users (available test users)")

        if concurrent_users < 5:
            print(f"‚ùå Need at least 5 test users for meaningful concurrent testing, have {available_users}")
            return
        
        # Prepare test scenarios
        test_scenarios = []
        for i in range(concurrent_users):
            user_id = str(self.test_user_ids[i % len(self.test_user_ids)])
            
            for j in range(requests_per_user):
                # Mix different endpoint types
                if j % 3 == 0:
                    scenario = {
                        "type": "model_status",
                        "method": "GET",
                        "url": f"{self.ml_api_base}/model-status",
                        "payload": None
                    }
                elif j % 3 == 1:
                    scenario = {
                        "type": "weakness_analysis",
                        "method": "POST",
                        "url": f"{self.ml_api_base}/analyze-weaknesses",
                        "payload": {
                            "user_id": user_id,
                            "include_confidence": True,
                            "analysis_depth": "standard"
                        }
                    }
                else:
                    scenario = {
                        "type": "training_recommendations",
                        "method": "POST", 
                        "url": f"{self.ml_api_base}/recommend-training",
                        "payload": {
                            "user_id": user_id,
                            "skill_level": "platinum",
                            "max_recommendations": 3
                        }
                    }
                
                test_scenarios.append(scenario)
        
        # Execute concurrent requests
        start_time = time.time()
        
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(limit=100, limit_per_host=50)
        ) as session:
            tasks = [self._execute_request(session, scenario) for scenario in test_scenarios]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_time = time.time() - start_time
        
        # Analyze results
        successful_results = [r for r in results if isinstance(r, dict) and not isinstance(r, Exception)]
        failed_results = [r for r in results if isinstance(r, Exception)]
        
        if successful_results:
            response_times = [r['response_time_ms'] for r in successful_results]
            
            metrics = {
                "total_requests": len(test_scenarios),
                "successful_requests": len(successful_results),
                "failed_requests": len(failed_results),
                "success_rate": len(successful_results) / len(test_scenarios),
                "avg_response_time_ms": statistics.mean(response_times),
                "median_response_time_ms": statistics.median(response_times),
                "p95_response_time_ms": sorted(response_times)[int(0.95 * len(response_times))],
                "p99_response_time_ms": sorted(response_times)[int(0.99 * len(response_times))],
                "max_response_time_ms": max(response_times),
                "requests_per_second": len(successful_results) / total_time,
                "total_test_duration_seconds": total_time,
                "concurrent_users": concurrent_users,
                "requests_per_user": requests_per_user
            }
            
            self.log_result("concurrent_load_tests", f"{concurrent_users}_concurrent_users", metrics)
        else:
            print(f"‚ùå All {len(test_scenarios)} requests failed")
    
    async def _execute_request(self, session: aiohttp.ClientSession, scenario: Dict) -> Dict:
        """Execute a single request scenario."""
        start_time = time.time()
        
        try:
            if scenario["method"] == "GET":
                async with session.get(scenario["url"]) as response:
                    await response.text()
                    status_code = response.status
            else:  # POST
                async with session.post(scenario["url"], json=scenario["payload"]) as response:
                    await response.text()
                    status_code = response.status
            
            response_time = (time.time() - start_time) * 1000
            
            return {
                "type": scenario["type"],
                "status_code": status_code,
                "response_time_ms": response_time,
                "success": status_code in [200, 400, 404]
            }
            
        except Exception as e:
            return {
                "type": scenario["type"],
                "error": str(e),
                "response_time_ms": (time.time() - start_time) * 1000,
                "success": False
            }
    
    async def cache_performance_test(self):
        """Test Redis cache performance under load."""
        print("\nüíæ Testing Cache Performance...")
        
        if not self.test_user_ids:
            print("‚ùå No test users available")
            return
        
        test_user_id = str(self.test_user_ids[0])
        
        # Test cache miss (first request)
        payload = {
            "user_id": test_user_id,
            "include_confidence": True,
            "analysis_depth": "standard"
        }
        
        async with aiohttp.ClientSession() as session:
            # First request (cache miss)
            start_time = time.time()
            async with session.post(f"{self.ml_api_base}/analyze-weaknesses", json=payload) as response:
                await response.text()
                cache_miss_time = (time.time() - start_time) * 1000
            
            # Second request (cache hit)
            start_time = time.time()
            async with session.post(f"{self.ml_api_base}/analyze-weaknesses", json=payload) as response:
                data = await response.json()
                cache_hit_time = (time.time() - start_time) * 1000
            
            # Multiple cache hits to test consistency
            cache_hit_times = []
            for _ in range(10):
                start_time = time.time()
                async with session.post(f"{self.ml_api_base}/analyze-weaknesses", json=payload) as response:
                    await response.text()
                    cache_hit_times.append((time.time() - start_time) * 1000)
        
        cache_improvement = ((cache_miss_time - cache_hit_time) / cache_miss_time) * 100

        metrics = {
            "cache_miss_time_ms": cache_miss_time,
            "cache_hit_time_ms": cache_hit_time,
            "avg_cache_hit_time_ms": statistics.mean(cache_hit_times),
            "cache_performance_improvement_percent": cache_improvement,
            "cache_consistency": statistics.stdev(cache_hit_times) < 10,  # Low variance
            "success_rate": 1.0,  # Cache test succeeded
            "avg_response_time_ms": statistics.mean(cache_hit_times)  # For reporting
        }

        self.log_result("cache_performance", "redis_cache_effectiveness", metrics)
    
    async def cleanup_test_data(self):
        """Clean up test data."""
        print(f"\nüßπ Cleaning up {len(self.test_user_ids)} test users...")
        
        try:
            from app.database import get_db
            from app.models.user import User
            from app.models.match import Match
            
            db = next(get_db())
            
            for user_id in self.test_user_ids:
                # Delete matches first (foreign key constraint)
                matches_deleted = db.query(Match).filter(Match.user_id == user_id).delete()
                
                # Delete user
                users_deleted = db.query(User).filter(User.id == user_id).delete()
            
            db.commit()
            print(f"‚úÖ Cleanup completed for {len(self.test_user_ids)} users")
            
        except Exception as e:
            print(f"‚ùå Cleanup failed: {e}")
            if 'db' in locals():
                db.rollback()
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """Generate comprehensive performance report."""
        print("\n" + "="*80)
        print("üìä COMPREHENSIVE PERFORMANCE TEST REPORT")
        print("="*80)
        
        # Overall assessment
        all_tests_passed = True
        critical_issues = []
        warnings = []
        
        for category, tests in self.results.items():
            if not tests:
                continue
                
            print(f"\n{category.upper().replace('_', ' ')}:")
            
            for test_name, metrics in tests.items():
                avg_time = metrics.get('avg_response_time_ms', 0)
                success_rate = metrics.get('success_rate', 0) * 100
                
                status = "‚úÖ PASS"
                if avg_time > 500:
                    status = "‚ùå FAIL"
                    all_tests_passed = False
                    critical_issues.append(f"{test_name}: {avg_time:.1f}ms (>500ms)")
                elif avg_time > 300:
                    status = "‚ö†Ô∏è  WARN"
                    warnings.append(f"{test_name}: {avg_time:.1f}ms (>300ms)")
                
                if success_rate < 95:
                    status = "‚ùå FAIL"
                    all_tests_passed = False
                    critical_issues.append(f"{test_name}: {success_rate:.1f}% success rate (<95%)")
                
                print(f"  {status} {test_name}")
                print(f"      Avg Response: {avg_time:.1f}ms")
                print(f"      Success Rate: {success_rate:.1f}%")
                
                if 'p95_response_time_ms' in metrics:
                    print(f"      P95 Response: {metrics['p95_response_time_ms']:.1f}ms")
                if 'requests_per_second' in metrics:
                    print(f"      Throughput: {metrics['requests_per_second']:.1f} req/sec")
        
        # Final assessment
        print("\n" + "="*80)
        if all_tests_passed:
            print("üéâ EXCELLENT: ML API is ready for production!")
            print("   All performance requirements met (<500ms, >95% success rate)")
        elif len(critical_issues) == 0:
            print("‚úÖ GOOD: ML API performance is acceptable with minor warnings")
            for warning in warnings:
                print(f"   ‚ö†Ô∏è  {warning}")
        else:
            print("‚ùå CRITICAL ISSUES: ML API needs optimization before production")
            for issue in critical_issues:
                print(f"   ‚ùå {issue}")
        
        print("="*80)
        
        return {
            "overall_status": "pass" if all_tests_passed else "fail",
            "critical_issues": critical_issues,
            "warnings": warnings,
            "detailed_results": self.results,
            "timestamp": datetime.now().isoformat()
        }
    
    async def run_full_performance_suite(self):
        """Run the complete performance testing suite."""
        print("üöÄ Starting Comprehensive ML API Performance Testing")
        print("="*80)
        
        try:
            # Setup
            user_ids = await self.create_test_users_with_data(20)
            if not user_ids:
                print("‚ùå Failed to create test data. Cannot proceed.")
                return {"overall_status": "setup_failed"}
            
            # Run tests
            await self.single_request_performance_test()
            await self.cache_performance_test()
            await self.concurrent_load_test(concurrent_users=25, requests_per_user=4)
            await self.concurrent_load_test(concurrent_users=50, requests_per_user=2)
            
            # Generate report
            report = self.generate_performance_report()
            
            # Save detailed report
            with open("ml_api_performance_report.json", "w") as f:
                json.dump(report, f, indent=2)
            
            print(f"\nüìÑ Detailed performance report saved to: ml_api_performance_report.json")
            
            return report
            
        finally:
            # Always cleanup
            await self.cleanup_test_data()


async def main():
    """Run comprehensive performance testing."""
    tester = PerformanceTestSuite()
    report = await tester.run_full_performance_suite()
    
    # Return appropriate exit code
    success = report.get("overall_status") == "pass"
    return 0 if success else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
